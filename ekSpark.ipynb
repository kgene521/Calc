{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ekSpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKzNtloSQbMlbC1vA2AfoA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgene521/Calc/blob/master/ekSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgBtBdgCF8P5"
      },
      "outputs": [],
      "source": [
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q http://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "# !wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
        "!mkdir spark321\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz -C spark321/\n",
        "!wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
        "!mv cars.csv spark321/\n",
        "!ls -la spark321/\n",
        "!cd spark321/;pwd\n",
        "!cd /content/\n",
        "!pip install -q findspark\n",
        "!cd /content/spark321/;pwd;ls -la\n",
        "!pip install pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark321/spark-3.2.1-bin-hadoop3.2\"\n",
        "!echo $SPARK_HOME"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", 'True') # Property used to format output tables better\n",
        "# spark\n",
        "df = spark.read.csv('/content/spark321/cars.csv', header=True, sep=\";\")\n",
        "# df.show(5, truncate=False)\n",
        "df2 = df.limit(5)\n",
        "# df2\n",
        "# df2.columns\n",
        "# df2.dtypes\n",
        "df2.printSchema\n",
        "df.limit(5)\n",
        "# Creating a list of the schema in the format column_name, data_type\n",
        "from pyspark.sql.types import *\n",
        "labels = [\n",
        "     ('Car',StringType()),\n",
        "     ('MPG',DoubleType()),\n",
        "     ('Cylinders',IntegerType()),\n",
        "     ('Displacement',DoubleType()),\n",
        "     ('Horsepower',DoubleType()),\n",
        "     ('Weight',DoubleType()),\n",
        "     ('Acceleration',DoubleType()),\n",
        "     ('Model',IntegerType()),\n",
        "     ('Origin',StringType())\n",
        "]\n",
        "# # Creating the schema that will be passed when reading the csv\n",
        "# print(labels)\n",
        "schema = StructType([StructField (x[0], x[1], True) for x in labels])\n",
        "# schema\n",
        "df = spark.read.csv('/content/spark321/cars.csv', header=True, sep=\";\", schema=schema)\n",
        "# df.printSchema()\n",
        "# df.show(truncate=False)\n",
        "# print(df.Car)\n",
        "# print(\"*\"*35)\n",
        "# df.select(df.Car).show(truncate=False)\n",
        "# print(df['car'])\n",
        "# print(\"*\"*20)\n",
        "# Column name is case sensitive\n",
        "# df.select(df['car']).show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "### Column name is case insensitive\n",
        "# df.select(col('car'), col('CAR')).show(truncate=False)\n",
        "### 1st method\n",
        "### Column name is case sensitive in this usage\n",
        "### print(df.Car, df.Cylinders)\n",
        "### print(\"*\"*40)\n",
        "# df.select(df.Car, df.Cylinders).show(truncate=False)\n",
        "### print(df2)\n",
        "### 2nd method\n",
        "### Column name is case insensitive in this usage\n",
        "### print(df['car'],df['cylinders'])\n",
        "### print(\"*\"*40)\n",
        "### df.select(df['caR'],df['cylINDers']).show(truncate=False)\n",
        "### 3rd method\n",
        "### Column name is case insensitive in this usage\n",
        "from pyspark.sql.functions import col\n",
        "# df.select(col('caR'),col('cylINDers')).show(truncate=False)\n",
        "### CASE 1: Adding a new column\n",
        "### We will add a new column called 'first_column' at the end\n",
        "from pyspark.sql.functions import lit\n",
        "df = df.withColumn('first_column',lit(1))\n",
        "### lit means literal. It populates the row with the literal value given.\n",
        "### When adding static data / constant values, it is a good practice to use it.\n",
        "# df.show(5,truncate=False)\n",
        "# CASE 2: Adding multiple columns\n",
        "# We will add two new columns called 'second_column' and 'third_column' at the end\n",
        "df = df.withColumn('second_column', lit(2)) \\\n",
        "       .withColumn('third_column', lit('Third Column'))\n",
        "# lit means literal. It populates the row with the literal value given.\n",
        "# When adding static data / constant values, it is a good practice to use it.\n",
        "# df.show(5,truncate=False)\n",
        "# CASE 3: Deriving a new column from an exisitng one\n",
        "# We will add a new column called 'car_model' which has the value of car and model appended together with a space in between \n",
        "from pyspark.sql.functions import concat\n",
        "df = df.withColumn('car_model', concat(col(\"Car\"), lit(\" \"), col(\"model\")))\n",
        "# lit means literal. It populates the row with the literal value given.\n",
        "# When adding static data / constant values, it is a good practice to use it.\n",
        "# df.show(5,truncate=False)\n",
        "#Renaming a column in PySpark\n",
        "df = df.withColumnRenamed('first_column', 'new_column_one') \\\n",
        "       .withColumnRenamed('second_column', 'new_column_two') \\\n",
        "       .withColumnRenamed('third_column', 'new_column_three')\n",
        "# df.show(truncate=False)\n",
        "\n",
        "# Group By a column in PySpark\n",
        "# df.groupBy('Origin').count().show(5)\n",
        "\n",
        "# Group By multiple columns in PySpark\n",
        "df.groupBy('Origin', 'Model').count().show(5)\n",
        "#Remove columns in PySpark\n",
        "df = df.drop('new_column_one')\n",
        "# df.show(5,truncate=False)\n",
        "\n",
        "#Remove multiple columnss in one go\n",
        "df = df.drop('new_column_two') \\\n",
        "       .drop('new_column_three')\n",
        "# df.show(5,truncate=False)\n",
        "\n",
        "\n",
        "# Filtering rows in PySpark\n",
        "total_count = df.count()\n",
        "print(\"TOTAL RECORD COUNT: \" + str(total_count))\n",
        "europe_filtered_count = df.filter(col('Origin')=='Europe').count()\n",
        "print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n",
        "# df.filter(col('Origin')=='Europe').show(truncate=False)\n",
        "# Filtering rows in PySpark based on Multiple conditions\n",
        "total_count = df.count()\n",
        "print(\"TOTAL RECORD COUNT: \" + str(total_count))\n",
        "europe_filtered_count = df.filter((col('Origin')=='Europe') &\n",
        "                                  (col('Cylinders')==4)).count() # Two conditions added here\n",
        "print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n",
        "# df.filter(col('Origin')=='Europe').show(truncate=False)\n",
        "\n",
        "#Get Unique Rows in PySpark\n",
        "# df.select('Origin').distinct().show()\n",
        "\n",
        "\n",
        "\n",
        "#Get Unique Rows in PySpark based on mutliple columns\n",
        "# df.select('Origin','model').distinct().show()\n",
        "\n",
        "\n",
        "\n",
        "# Sort Rows in PySpark\n",
        "# By default the data will be sorted in ascending order\n",
        "# df.orderBy('Cylinders').show(5, truncate=False)\n",
        "# df.orderBy('Horsepower', ascending=False).show(5, truncate=False)\n",
        "\n",
        "# Using groupBy aand orderBy together\n",
        "df.groupBy(\"Origin\").count().orderBy('count', ascending=False).show(10)\n",
        "\n",
        "# CASE 1: Union When columns are in order\n",
        "europe_cars = df.filter((col('Origin')=='Europe') & (col('Cylinders')==5))\n",
        "japan_cars = df.filter((col('Origin')=='Japan') & (col('Cylinders')==3))\n",
        "print(\"EUROPE CARS: \"+str(europe_cars.count()))\n",
        "print(\"JAPAN CARS: \"+str(japan_cars.count()))\n",
        "print(\"AFTER UNION: \"+str(europe_cars.union(japan_cars).count()))\n",
        "\n",
        "# CASE 1: Union When columns are not in order\n",
        "# Creating two dataframes with jumbled columns\n",
        "df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
        "df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
        "df1.unionByName(df2).show()\n",
        "\n",
        "# Functions available in PySpark\n",
        "from pyspark.sql import functions\n",
        "# Similar to python, we can use the dir function to view the avaiable functions\n",
        "foos = dir(functions)\n",
        "counter = 0\n",
        "for x in foos:\n",
        "  counter = counter + 1\n",
        "  if x[0] == '_' or counter <= 8:\n",
        "    continue\n",
        "  print(f\"x: {x}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1bIASJN9KgUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foos = dir(functions)\n",
        "function_list = \"\"\n",
        "counter = 0\n",
        "for x in foos:\n",
        "  counter = counter + 1\n",
        "  if x[0] == '_' or counter <= 8:\n",
        "    continue\n",
        "  if counter % 10 == 0:\n",
        "    print(function_list)\n",
        "    function_list = \"\"\n",
        "  else:\n",
        "    function_list = function_list + \", \" + x\n",
        "print(function_list)\n",
        "# print(foos)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OTsQ-3b8B1nO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927fee00-ddfc-4f0e-8e4a-547f2af2bfe2"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", abs, acos, acosh, add_months, aggregate, approxCountDistinct, approx_count_distinct, array, array_contains\n",
            ", array_except, array_intersect, array_join, array_max, array_min, array_position, array_remove, array_repeat, array_sort\n",
            ", arrays_overlap, arrays_zip, asc, asc_nulls_first, asc_nulls_last, ascii, asin, asinh, assert_true\n",
            ", atan2, atanh, avg, base64, bin, bitwiseNOT, bitwise_not, broadcast, bround\n",
            ", cbrt, ceil, coalesce, col, collect_list, collect_set, column, concat, concat_ws\n",
            ", corr, cos, cosh, count, countDistinct, count_distinct, covar_pop, covar_samp, crc32\n",
            ", cume_dist, current_date, current_timestamp, date_add, date_format, date_sub, date_trunc, datediff, dayofmonth\n",
            ", dayofyear, days, decode, degrees, dense_rank, desc, desc_nulls_first, desc_nulls_last, element_at\n",
            ", exists, exp, explode, explode_outer, expm1, expr, factorial, filter, first\n",
            ", floor, forall, format_number, format_string, from_csv, from_json, from_unixtime, from_utc_timestamp, functools\n",
            ", greatest, grouping, grouping_id, hash, hex, hour, hours, hypot, initcap\n",
            ", instr, isnan, isnull, json_tuple, kurtosis, lag, last, last_day, lead\n",
            ", length, levenshtein, lit, locate, log, log10, log1p, log2, lower\n",
            ", ltrim, map_concat, map_entries, map_filter, map_from_arrays, map_from_entries, map_keys, map_values, map_zip_with\n",
            ", md5, mean, min, minute, monotonically_increasing_id, month, months, months_between, nanvl\n",
            ", nth_value, ntile, overlay, pandas_udf, percent_rank, percentile_approx, posexplode, posexplode_outer, pow\n",
            ", quarter, radians, raise_error, rand, randn, rank, regexp_extract, regexp_replace, repeat\n",
            ", rint, round, row_number, rpad, rtrim, schema_of_csv, schema_of_json, second, sentences\n",
            ", session_window, sha1, sha2, shiftLeft, shiftRight, shiftRightUnsigned, shiftleft, shiftright, shiftrightunsigned\n",
            ", signum, sin, since, sinh, size, skewness, slice, sort_array, soundex\n",
            ", split, sqrt, stddev, stddev_pop, stddev_samp, struct, substring, substring_index, sum\n",
            ", sum_distinct, sys, tan, tanh, timestamp_seconds, toDegrees, toRadians, to_csv, to_date\n",
            ", to_str, to_timestamp, to_utc_timestamp, transform, transform_keys, transform_values, translate, trim, trunc\n",
            ", unbase64, unhex, unix_timestamp, upper, var_pop, var_samp, variance, warnings, weekofyear\n",
            ", window, xxhash64, year, years, zip_with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M6NqFrJ7ab10"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}