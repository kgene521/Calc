{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ekSpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8IEYML2xfdOOfnD5aMc2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgene521/Calc/blob/master/ekSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ZgBtBdgCF8P5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee717d0-d136-417c-aacd-1c729e2cf1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-18 06:32:06--  https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
            "Resolving jacobceles.github.io (jacobceles.github.io)... 185.199.111.153, 185.199.108.153, 185.199.110.153, ...\n",
            "Connecting to jacobceles.github.io (jacobceles.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/cars.csv [following]\n",
            "--2022-02-18 06:32:06--  https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/cars.csv\n",
            "Resolving jacobcelestine.com (jacobcelestine.com)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to jacobcelestine.com (jacobcelestine.com)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22608 (22K) [text/csv]\n",
            "Saving to: ‘cars.csv’\n",
            "\n",
            "cars.csv            100%[===================>]  22.08K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-02-18 06:32:06 (22.0 MB/s) - ‘cars.csv’ saved [22608/22608]\n",
            "\n",
            "mkdir: cannot create directory ‘spark321’: File exists\n",
            "--2022-02-18 06:32:10--  https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
            "Resolving jacobceles.github.io (jacobceles.github.io)... 185.199.111.153, 185.199.108.153, 185.199.110.153, ...\n",
            "Connecting to jacobceles.github.io (jacobceles.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/cars.csv [following]\n",
            "--2022-02-18 06:32:10--  https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/cars.csv\n",
            "Resolving jacobcelestine.com (jacobcelestine.com)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to jacobcelestine.com (jacobcelestine.com)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22608 (22K) [text/csv]\n",
            "Saving to: ‘cars.csv.2’\n",
            "\n",
            "cars.csv.2          100%[===================>]  22.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-18 06:32:10 (85.1 MB/s) - ‘cars.csv.2’ saved [22608/22608]\n",
            "\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n",
            "/content/spark321/spark-3.2.1-bin-hadoop3.2\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
        "!mkdir spark321\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz -C spark321/\n",
        "!wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
        "!mv cars.csv spark321/\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark321/spark-3.2.1-bin-hadoop3.2\"\n",
        "!echo $SPARK_HOME"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", 'True') # Property used to format output tables better\n",
        "# spark\n",
        "df = spark.read.csv('/content/spark321/cars.csv', header=True, sep=\";\")\n",
        "# df.show(5, truncate=False)\n",
        "df2 = df.limit(5)\n",
        "# df2\n",
        "# df2.columns\n",
        "# df2.dtypes\n",
        "df2.printSchema\n",
        "df.limit(5)\n",
        "# Creating a list of the schema in the format column_name, data_type\n",
        "from pyspark.sql.types import *\n",
        "labels = [\n",
        "     ('Car',StringType()),\n",
        "     ('MPG',DoubleType()),\n",
        "     ('Cylinders',IntegerType()),\n",
        "     ('Displacement',DoubleType()),\n",
        "     ('Horsepower',DoubleType()),\n",
        "     ('Weight',DoubleType()),\n",
        "     ('Acceleration',DoubleType()),\n",
        "     ('Model',IntegerType()),\n",
        "     ('Origin',StringType())\n",
        "]\n",
        "# # Creating the schema that will be passed when reading the csv\n",
        "# print(labels)\n",
        "schema = StructType([StructField (x[0], x[1], True) for x in labels])\n",
        "# schema\n",
        "df = spark.read.csv('/content/spark321/cars.csv', header=True, sep=\";\", schema=schema)\n",
        "# df.printSchema()\n",
        "# df.show(truncate=False)\n",
        "# print(df.Car)\n",
        "# print(\"*\"*35)\n",
        "# df.select(df.Car).show(truncate=False)\n",
        "# print(df['car'])\n",
        "# print(\"*\"*20)\n",
        "# Column name is case sensitive\n",
        "# df.select(df['car']).show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "### Column name is case insensitive\n",
        "# df.select(col('car'), col('CAR')).show(truncate=False)\n",
        "### 1st method\n",
        "### Column name is case sensitive in this usage\n",
        "### print(df.Car, df.Cylinders)\n",
        "### print(\"*\"*40)\n",
        "# df.select(df.Car, df.Cylinders).show(truncate=False)\n",
        "### print(df2)\n",
        "### 2nd method\n",
        "### Column name is case insensitive in this usage\n",
        "### print(df['car'],df['cylinders'])\n",
        "### print(\"*\"*40)\n",
        "### df.select(df['caR'],df['cylINDers']).show(truncate=False)\n",
        "### 3rd method\n",
        "### Column name is case insensitive in this usage\n",
        "from pyspark.sql.functions import col\n",
        "# df.select(col('caR'),col('cylINDers')).show(truncate=False)\n",
        "### CASE 1: Adding a new column\n",
        "### We will add a new column called 'first_column' at the end\n",
        "from pyspark.sql.functions import lit\n",
        "df = df.withColumn('first_column',lit(1))\n",
        "### lit means literal. It populates the row with the literal value given.\n",
        "### When adding static data / constant values, it is a good practice to use it.\n",
        "# df.show(5,truncate=False)\n",
        "# CASE 2: Adding multiple columns\n",
        "# We will add two new columns called 'second_column' and 'third_column' at the end\n",
        "df = df.withColumn('second_column', lit(2)) \\\n",
        "       .withColumn('third_column', lit('Third Column'))\n",
        "# lit means literal. It populates the row with the literal value given.\n",
        "# When adding static data / constant values, it is a good practice to use it.\n",
        "# df.show(5,truncate=False)\n",
        "# CASE 3: Deriving a new column from an exisitng one\n",
        "# We will add a new column called 'car_model' which has the value of car and model appended together with a space in between \n",
        "from pyspark.sql.functions import concat\n",
        "df = df.withColumn('car_model', concat(col(\"Car\"), lit(\" \"), col(\"model\")))\n",
        "# lit means literal. It populates the row with the literal value given.\n",
        "# When adding static data / constant values, it is a good practice to use it.\n",
        "# df.show(5,truncate=False)\n",
        "#Renaming a column in PySpark\n",
        "df = df.withColumnRenamed('first_column', 'new_column_one') \\\n",
        "       .withColumnRenamed('second_column', 'new_column_two') \\\n",
        "       .withColumnRenamed('third_column', 'new_column_three')\n",
        "# df.show(truncate=False)\n",
        "\n",
        "# Group By a column in PySpark\n",
        "# df.groupBy('Origin').count().show(5)\n",
        "\n",
        "# Group By multiple columns in PySpark\n",
        "# df.groupBy('Origin', 'Model').count().show(5)\n",
        "#Remove columns in PySpark\n",
        "df = df.drop('new_column_one')\n",
        "# df.show(5,truncate=False)\n",
        "\n",
        "#Remove multiple columnss in one go\n",
        "df = df.drop('new_column_two') \\\n",
        "       .drop('new_column_three')\n",
        "# df.show(5,truncate=False)\n",
        "\n",
        "\n",
        "# Filtering rows in PySpark\n",
        "total_count = df.count()\n",
        "# print(\"TOTAL RECORD COUNT: \" + str(total_count))\n",
        "europe_filtered_count = df.filter(col('Origin')=='Europe').count()\n",
        "# print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n",
        "# df.filter(col('Origin')=='Europe').show(truncate=False)\n",
        "# Filtering rows in PySpark based on Multiple conditions\n",
        "total_count = df.count()\n",
        "# print(\"TOTAL RECORD COUNT: \" + str(total_count))\n",
        "europe_filtered_count = df.filter((col('Origin')=='Europe') &\n",
        "                                  (col('Cylinders')==4)).count() # Two conditions added here\n",
        "# print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n",
        "# df.filter(col('Origin')=='Europe').show(truncate=False)\n",
        "\n",
        "#Get Unique Rows in PySpark\n",
        "# df.select('Origin').distinct().show()\n",
        "\n",
        "\n",
        "\n",
        "#Get Unique Rows in PySpark based on mutliple columns\n",
        "# df.select('Origin','model').distinct().show()\n",
        "\n",
        "\n",
        "\n",
        "# Sort Rows in PySpark\n",
        "# By default the data will be sorted in ascending order\n",
        "# df.orderBy('Cylinders').show(5, truncate=False)\n",
        "# df.orderBy('Horsepower', ascending=False).show(5, truncate=False)\n",
        "\n",
        "# Using groupBy aand orderBy together\n",
        "# df.groupBy(\"Origin\").count().orderBy('count', ascending=False).show(10)\n",
        "\n",
        "# CASE 1: Union When columns are in order\n",
        "europe_cars = df.filter((col('Origin')=='Europe') & (col('Cylinders')==5))\n",
        "japan_cars = df.filter((col('Origin')=='Japan') & (col('Cylinders')==3))\n",
        "# print(\"EUROPE CARS: \"+str(europe_cars.count()))\n",
        "# print(\"JAPAN CARS: \"+str(japan_cars.count()))\n",
        "# print(\"AFTER UNION: \"+str(europe_cars.union(japan_cars).count()))\n",
        "\n",
        "# CASE 1: Union When columns are not in order\n",
        "# Creating two dataframes with jumbled columns\n",
        "df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
        "df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
        "# df1.unionByName(df2).show()\n",
        "\n",
        "# Functions available in PySpark ######################### Functions in Spark ###\n",
        "from pyspark.sql import functions\n",
        "# Similar to python, we can use the dir function to view the avaiable functions\n",
        "def print_functions():\n",
        "  foos = dir(functions)\n",
        "  function_list = \"\"\n",
        "  counter = 0\n",
        "  for x in foos:\n",
        "    counter = counter + 1\n",
        "    if x[0] == '_' or counter <= 8:\n",
        "      continue\n",
        "    if counter % 10 == 0:\n",
        "      print(function_list)\n",
        "      function_list = \"\"\n",
        "    else:\n",
        "      function_list = function_list + \", \" + x\n",
        "  print(function_list)\n",
        "  # print(foos)\n",
        "# print_functions()\n",
        "\n",
        "# Loading the data\n",
        "from pyspark.sql.functions import col\n",
        "df = spark.read.csv('/content/spark321/cars.csv', header=True, sep=\";\", inferSchema=True)\n",
        "\n",
        "from pyspark.sql.functions import col,lower, upper, substring\n",
        "### Prints out the details of a function\n",
        "###       help(substring)  ### HELP on the substring() function\n",
        "### alias is used to rename the column name in the output\n",
        "# df.select(col('Car'),\n",
        "#           lower(col('Car')),\n",
        "#           upper(col('Car')),\n",
        "#           substring(col('Car'),1,4)\n",
        "#           .alias(\"concatenated value\")\n",
        "#           ).show(5, False)\n",
        "\n",
        "from pyspark.sql.functions import concat\n",
        "\n",
        "# df.select(col(\"Car\"),\n",
        "#           col(\"model\"),\n",
        "#           concat(col(\"Car\"), lit(\" \"), col(\"model\"))\n",
        "#           ).show(5, False)\n",
        "\n",
        "from pyspark.sql.functions import min, max\n",
        "\n",
        "# df.select(min(col('Weight')), max(col('Weight'))).show()\n",
        "\n",
        "from pyspark.sql.functions import min, max, lit\n",
        "\n",
        "# df.select(min(col('Weight'))+lit(10), max(col('Weight')+lit(10))).show()\n",
        "\n",
        "from pyspark.sql.functions import to_date, to_timestamp, lit\n",
        "from datetime import datetime\n",
        "\n",
        "df = spark.createDataFrame([('2019-12-25 13:30:00',)], ['DOB'])\n",
        "# df.show()\n",
        "# df.printSchema()\n",
        "\n",
        "###########################  '2019-12-25 13:30:33',\n",
        "df = spark.createDataFrame([(datetime.now()       ,)], ['DOB'])\n",
        "df = df.select(to_date(col('DOB'),'yyyy-MM-dd HH:mm:ss'), to_timestamp(col('DOB'),'yyyy-MM-dd HH:mm:ss'))\n",
        "# df.show()\n",
        "# df.printSchema()\n",
        "\n",
        "df = spark.createDataFrame([('25/Dec/2019 13:30:00',)], ['DOB'])\n",
        "df = df.select(to_date(col('DOB'),'dd/MMM/yyyy HH:mm:ss'), to_timestamp(col('DOB'),'dd/MMM/yyyy HH:mm:ss'))\n",
        "# df.show()\n",
        "# df.printSchema()\n",
        "\n",
        "### What is 3 days earlier that the oldest date and 3 days later than the most recent date?\n",
        "\n",
        "from pyspark.sql.functions import date_add, date_sub\n",
        "\n",
        "# create a dummy dataframe\n",
        "df = spark.createDataFrame([('1990-01-01',),('1995-01-03',),('2021-03-30',)], ['Date'])\n",
        "# find out the required dates\n",
        "# df.select(date_add(max(col('Date')),3),\n",
        "#           date_sub(min(col('Date')),3)\n",
        "#           ).show()\n",
        "\n",
        "### Joins in PySpark\n",
        "\n",
        "print('*'*10, \"Create two dataframes \", '*'*10)\n",
        "cars_df = spark.createDataFrame([[1, 'Car A'],[2, 'Car B'],[3, 'Car C'], [4, 'Car D']], [\"id\", \"car_name\"])\n",
        "car_price_df = spark.createDataFrame([[1, 1000],[2, 2000],[3, 3000]], [\"id\", \"car_price\"])\n",
        "cars_df.show()\n",
        "car_price_df.show()\n",
        "\n",
        "# Executing an inner join so we can see the id, name and price of each car in one row\n",
        "cdf = cars_df.join(car_price_df, cars_df.id == car_price_df.id, 'inner').select(cars_df['id'],cars_df['car_name'],car_price_df['car_price']).show(truncate=False)\n",
        "\n",
        "# cdf.orderBy(cdf.cars_df['car_price'], ascending=False)\n",
        "# cdf.show(truncate=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1bIASJN9KgUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a4a3be7a-2c74-4828-c1fd-5f081a48d323"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********** Create two dataframes  **********\n",
            "+---+--------+\n",
            "| id|car_name|\n",
            "+---+--------+\n",
            "|  1|   Car A|\n",
            "|  2|   Car B|\n",
            "|  3|   Car C|\n",
            "|  4|   Car D|\n",
            "+---+--------+\n",
            "\n",
            "+---+---------+\n",
            "| id|car_price|\n",
            "+---+---------+\n",
            "|  1|     1000|\n",
            "|  2|     2000|\n",
            "|  3|     3000|\n",
            "+---+---------+\n",
            "\n",
            "+---+--------+---------+\n",
            "|id |car_name|car_price|\n",
            "+---+--------+---------+\n",
            "|1  |Car A   |1000     |\n",
            "|2  |Car B   |2000     |\n",
            "|3  |Car C   |3000     |\n",
            "+---+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Executing an inner join so we can see the id, name and price of each car in one row\n",
        "cars_df.join(car_price_df, cars_df.id == car_price_df.id, 'inner').select(cars_df['id'],cars_df['car_name'],car_price_df['car_price']).show(truncate=False)\n"
      ],
      "metadata": {
        "id": "OTsQ-3b8B1nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M6NqFrJ7ab10"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}