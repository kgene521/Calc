{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ekSpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNN6HXYGZ12MgFL05Lzr/qy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgene521/Calc/blob/master/ekSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgBtBdgCF8P5"
      },
      "outputs": [],
      "source": [
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q http://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "# !wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
        "# !mkdir spark321\n",
        "# !tar xf spark-3.2.1-bin-hadoop3.2.tgz -C spark321/\n",
        "# !wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
        "# !mv cars.csv spark321/\n",
        "# !pip install -q findspark\n",
        "# !pip install pyspark\n",
        "# import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark321/spark-3.2.1-bin-hadoop3.2\"\n",
        "# !echo $SPARK_HOME"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", 'True') # Property used to format output tables better\n",
        "# spark\n",
        "df = spark.read.csv('/content/spark321/cars.csv', header=True, sep=\";\")\n",
        "# df.show(5, truncate=False)\n",
        "df2 = df.limit(5)\n",
        "# df2\n",
        "# df2.columns\n",
        "# df2.dtypes\n",
        "df2.printSchema\n",
        "df.limit(5)\n",
        "# Creating a list of the schema in the format column_name, data_type\n",
        "from pyspark.sql.types import *\n",
        "labels = [\n",
        "     ('Car',StringType()),\n",
        "     ('MPG',DoubleType()),\n",
        "     ('Cylinders',IntegerType()),\n",
        "     ('Displacement',DoubleType()),\n",
        "     ('Horsepower',DoubleType()),\n",
        "     ('Weight',DoubleType()),\n",
        "     ('Acceleration',DoubleType()),\n",
        "     ('Model',IntegerType()),\n",
        "     ('Origin',StringType())\n",
        "]\n",
        "# # Creating the schema that will be passed when reading the csv\n",
        "# print(labels)\n",
        "schema = StructType([StructField (x[0], x[1], True) for x in labels])\n",
        "# schema\n",
        "df = spark.read.csv('/content/spark321/cars.csv', header=True, sep=\";\", schema=schema)\n",
        "# df.printSchema()\n",
        "# df.show(truncate=False)\n",
        "# print(df.Car)\n",
        "# print(\"*\"*35)\n",
        "# df.select(df.Car).show(truncate=False)\n",
        "# print(df['car'])\n",
        "# print(\"*\"*20)\n",
        "# Column name is case sensitive\n",
        "# df.select(df['car']).show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "### Column name is case insensitive\n",
        "# df.select(col('car'), col('CAR')).show(truncate=False)\n",
        "### 1st method\n",
        "### Column name is case sensitive in this usage\n",
        "### print(df.Car, df.Cylinders)\n",
        "### print(\"*\"*40)\n",
        "# df.select(df.Car, df.Cylinders).show(truncate=False)\n",
        "### print(df2)\n",
        "### 2nd method\n",
        "### Column name is case insensitive in this usage\n",
        "### print(df['car'],df['cylinders'])\n",
        "### print(\"*\"*40)\n",
        "### df.select(df['caR'],df['cylINDers']).show(truncate=False)\n",
        "### 3rd method\n",
        "### Column name is case insensitive in this usage\n",
        "from pyspark.sql.functions import col\n",
        "# df.select(col('caR'),col('cylINDers')).show(truncate=False)\n",
        "### CASE 1: Adding a new column\n",
        "### We will add a new column called 'first_column' at the end\n",
        "from pyspark.sql.functions import lit\n",
        "df = df.withColumn('first_column',lit(1))\n",
        "### lit means literal. It populates the row with the literal value given.\n",
        "### When adding static data / constant values, it is a good practice to use it.\n",
        "# df.show(5,truncate=False)\n",
        "# CASE 2: Adding multiple columns\n",
        "# We will add two new columns called 'second_column' and 'third_column' at the end\n",
        "df = df.withColumn('second_column', lit(2)) \\\n",
        "       .withColumn('third_column', lit('Third Column'))\n",
        "# lit means literal. It populates the row with the literal value given.\n",
        "# When adding static data / constant values, it is a good practice to use it.\n",
        "# df.show(5,truncate=False)\n",
        "# CASE 3: Deriving a new column from an exisitng one\n",
        "# We will add a new column called 'car_model' which has the value of car and model appended together with a space in between \n",
        "from pyspark.sql.functions import concat\n",
        "df = df.withColumn('car_model', concat(col(\"Car\"), lit(\" \"), col(\"model\")))\n",
        "# lit means literal. It populates the row with the literal value given.\n",
        "# When adding static data / constant values, it is a good practice to use it.\n",
        "# df.show(5,truncate=False)\n",
        "#Renaming a column in PySpark\n",
        "df = df.withColumnRenamed('first_column', 'new_column_one') \\\n",
        "       .withColumnRenamed('second_column', 'new_column_two') \\\n",
        "       .withColumnRenamed('third_column', 'new_column_three')\n",
        "# df.show(truncate=False)\n",
        "\n",
        "# Group By a column in PySpark\n",
        "# df.groupBy('Origin').count().show(5)\n",
        "\n",
        "# Group By multiple columns in PySpark\n",
        "# df.groupBy('Origin', 'Model').count().show(5)\n",
        "#Remove columns in PySpark\n",
        "df = df.drop('new_column_one')\n",
        "# df.show(5,truncate=False)\n",
        "\n",
        "#Remove multiple columnss in one go\n",
        "df = df.drop('new_column_two') \\\n",
        "       .drop('new_column_three')\n",
        "# df.show(5,truncate=False)\n",
        "\n",
        "\n",
        "# Filtering rows in PySpark\n",
        "total_count = df.count()\n",
        "# print(\"TOTAL RECORD COUNT: \" + str(total_count))\n",
        "europe_filtered_count = df.filter(col('Origin')=='Europe').count()\n",
        "# print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n",
        "# df.filter(col('Origin')=='Europe').show(truncate=False)\n",
        "# Filtering rows in PySpark based on Multiple conditions\n",
        "total_count = df.count()\n",
        "# print(\"TOTAL RECORD COUNT: \" + str(total_count))\n",
        "europe_filtered_count = df.filter((col('Origin')=='Europe') &\n",
        "                                  (col('Cylinders')==4)).count() # Two conditions added here\n",
        "# print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n",
        "# df.filter(col('Origin')=='Europe').show(truncate=False)\n",
        "\n",
        "#Get Unique Rows in PySpark\n",
        "# df.select('Origin').distinct().show()\n",
        "\n",
        "\n",
        "\n",
        "#Get Unique Rows in PySpark based on mutliple columns\n",
        "# df.select('Origin','model').distinct().show()\n",
        "\n",
        "\n",
        "\n",
        "# Sort Rows in PySpark\n",
        "# By default the data will be sorted in ascending order\n",
        "# df.orderBy('Cylinders').show(5, truncate=False)\n",
        "# df.orderBy('Horsepower', ascending=False).show(5, truncate=False)\n",
        "\n",
        "# Using groupBy aand orderBy together\n",
        "# df.groupBy(\"Origin\").count().orderBy('count', ascending=False).show(10)\n",
        "\n",
        "# CASE 1: Union When columns are in order\n",
        "europe_cars = df.filter((col('Origin')=='Europe') & (col('Cylinders')==5))\n",
        "japan_cars = df.filter((col('Origin')=='Japan') & (col('Cylinders')==3))\n",
        "# print(\"EUROPE CARS: \"+str(europe_cars.count()))\n",
        "# print(\"JAPAN CARS: \"+str(japan_cars.count()))\n",
        "# print(\"AFTER UNION: \"+str(europe_cars.union(japan_cars).count()))\n",
        "\n",
        "# CASE 1: Union When columns are not in order\n",
        "# Creating two dataframes with jumbled columns\n",
        "df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
        "df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
        "# df1.unionByName(df2).show()\n",
        "\n",
        "# Functions available in PySpark ######################### Functions in Spark ###\n",
        "from pyspark.sql import functions\n",
        "# Similar to python, we can use the dir function to view the avaiable functions\n",
        "def print_functions():\n",
        "  foos = dir(functions)\n",
        "  function_list = \"\"\n",
        "  counter = 0\n",
        "  for x in foos:\n",
        "    counter = counter + 1\n",
        "    if x[0] == '_' or counter <= 8:\n",
        "      continue\n",
        "    if counter % 10 == 0:\n",
        "      print(function_list)\n",
        "      function_list = \"\"\n",
        "    else:\n",
        "      function_list = function_list + \", \" + x\n",
        "  print(function_list)\n",
        "  # print(foos)\n",
        "# print_functions()\n",
        "\n",
        "# Loading the data\n",
        "from pyspark.sql.functions import col\n",
        "df = spark.read.csv('/content/spark321/cars.csv', header=True, sep=\";\", inferSchema=True)\n",
        "\n",
        "from pyspark.sql.functions import col,lower, upper, substring\n",
        "### Prints out the details of a function\n",
        "###       help(substring)  ### HELP on the substring() function\n",
        "### alias is used to rename the column name in the output\n",
        "# df.select(col('Car'),\n",
        "#           lower(col('Car')),\n",
        "#           upper(col('Car')),\n",
        "#           substring(col('Car'),1,4)\n",
        "#           .alias(\"concatenated value\")\n",
        "#           ).show(5, False)\n",
        "\n",
        "from pyspark.sql.functions import concat\n",
        "\n",
        "# df.select(col(\"Car\"),\n",
        "#           col(\"model\"),\n",
        "#           concat(col(\"Car\"), lit(\" \"), col(\"model\"))\n",
        "#           ).show(5, False)\n",
        "\n",
        "from pyspark.sql.functions import min, max\n",
        "\n",
        "# df.select(min(col('Weight')), max(col('Weight'))).show()\n",
        "\n",
        "from pyspark.sql.functions import min, max, lit\n",
        "\n",
        "# df.select(min(col('Weight'))+lit(10), max(col('Weight')+lit(10))).show()\n",
        "\n",
        "from pyspark.sql.functions import to_date, to_timestamp, lit\n",
        "\n",
        "df = spark.createDataFrame([('2019-12-25 13:30:00',)], ['DOB'])\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1bIASJN9KgUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653eed96-a9b2-4740-cf1d-7358c524bd39"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|                DOB|\n",
            "+-------------------+\n",
            "|2019-12-25 13:30:00|\n",
            "+-------------------+\n",
            "\n",
            "root\n",
            " |-- DOB: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "OTsQ-3b8B1nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M6NqFrJ7ab10"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}