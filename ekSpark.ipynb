{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ekSpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNit8PbXXFqOlmqgFgxpZS+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgene521/Calc/blob/master/ekSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgBtBdgCF8P5"
      },
      "outputs": [],
      "source": [
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q http://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "# !wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
        "!mkdir spark321\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz -C spark321/\n",
        "!wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
        "!mv cars.csv spark321/\n",
        "!ls -la spark321/\n",
        "!cd spark321/;pwd\n",
        "!cd /content/\n",
        "!pip install -q findspark\n",
        "!cd /content/spark321/;pwd;ls -la\n",
        "!pip install pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark321/spark-3.2.1-bin-hadoop3.2\"\n",
        "!echo $SPARK_HOME"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", 'True') # Property used to format output tables better\n",
        "# spark\n",
        "df = spark.read.csv('/content/spark321/cars.csv', header=True, sep=\";\")\n",
        "# df.show(5, truncate=False)\n",
        "df2 = df.limit(5)\n",
        "# df2\n",
        "# df2.columns\n",
        "# df2.dtypes\n",
        "df2.printSchema\n",
        "df.limit(5)\n",
        "# Creating a list of the schema in the format column_name, data_type\n",
        "from pyspark.sql.types import *\n",
        "labels = [\n",
        "     ('Car',StringType()),\n",
        "     ('MPG',DoubleType()),\n",
        "     ('Cylinders',IntegerType()),\n",
        "     ('Displacement',DoubleType()),\n",
        "     ('Horsepower',DoubleType()),\n",
        "     ('Weight',DoubleType()),\n",
        "     ('Acceleration',DoubleType()),\n",
        "     ('Model',IntegerType()),\n",
        "     ('Origin',StringType())\n",
        "]\n",
        "# # Creating the schema that will be passed when reading the csv\n",
        "# print(labels)\n",
        "schema = StructType([StructField (x[0], x[1], True) for x in labels])\n",
        "# schema\n",
        "df = spark.read.csv('/content/spark321/cars.csv', header=True, sep=\";\", schema=schema)\n",
        "# df.printSchema()\n",
        "# df.show(truncate=False)\n",
        "# print(df.Car)\n",
        "# print(\"*\"*35)\n",
        "# df.select(df.Car).show(truncate=False)\n",
        "# print(df['car'])\n",
        "# print(\"*\"*20)\n",
        "# Column name is case sensitive\n",
        "# df.select(df['car']).show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "### Column name is case insensitive\n",
        "# df.select(col('car'), col('CAR')).show(truncate=False)\n",
        "### 1st method\n",
        "### Column name is case sensitive in this usage\n",
        "### print(df.Car, df.Cylinders)\n",
        "### print(\"*\"*40)\n",
        "# df.select(df.Car, df.Cylinders).show(truncate=False)\n",
        "### print(df2)\n",
        "### 2nd method\n",
        "### Column name is case insensitive in this usage\n",
        "### print(df['car'],df['cylinders'])\n",
        "### print(\"*\"*40)\n",
        "### df.select(df['caR'],df['cylINDers']).show(truncate=False)\n",
        "### 3rd method\n",
        "### Column name is case insensitive in this usage\n",
        "from pyspark.sql.functions import col\n",
        "# df.select(col('caR'),col('cylINDers')).show(truncate=False)\n",
        "### CASE 1: Adding a new column\n",
        "### We will add a new column called 'first_column' at the end\n",
        "from pyspark.sql.functions import lit\n",
        "df = df.withColumn('first_column',lit(1))\n",
        "### lit means literal. It populates the row with the literal value given.\n",
        "### When adding static data / constant values, it is a good practice to use it.\n",
        "# df.show(5,truncate=False)\n",
        "# CASE 2: Adding multiple columns\n",
        "# We will add two new columns called 'second_column' and 'third_column' at the end\n",
        "df = df.withColumn('second_column', lit(2)) \\\n",
        "       .withColumn('third_column', lit('Third Column'))\n",
        "# lit means literal. It populates the row with the literal value given.\n",
        "# When adding static data / constant values, it is a good practice to use it.\n",
        "# df.show(5,truncate=False)\n",
        "# CASE 3: Deriving a new column from an exisitng one\n",
        "# We will add a new column called 'car_model' which has the value of car and model appended together with a space in between \n",
        "from pyspark.sql.functions import concat\n",
        "df = df.withColumn('car_model', concat(col(\"Car\"), lit(\" \"), col(\"model\")))\n",
        "# lit means literal. It populates the row with the literal value given.\n",
        "# When adding static data / constant values, it is a good practice to use it.\n",
        "# df.show(5,truncate=False)\n",
        "#Renaming a column in PySpark\n",
        "df = df.withColumnRenamed('first_column', 'new_column_one') \\\n",
        "       .withColumnRenamed('second_column', 'new_column_two') \\\n",
        "       .withColumnRenamed('third_column', 'new_column_three')\n",
        "# df.show(truncate=False)\n",
        "\n",
        "# Group By a column in PySpark\n",
        "# df.groupBy('Origin').count().show(5)\n",
        "\n",
        "# Group By multiple columns in PySpark\n",
        "df.groupBy('Origin', 'Model').count().show(5)\n",
        "#Remove columns in PySpark\n",
        "df = df.drop('new_column_one')\n",
        "# df.show(5,truncate=False)\n",
        "\n",
        "#Remove multiple columnss in one go\n",
        "df = df.drop('new_column_two') \\\n",
        "       .drop('new_column_three')\n",
        "# df.show(5,truncate=False)\n",
        "\n",
        "\n",
        "# Filtering rows in PySpark\n",
        "total_count = df.count()\n",
        "print(\"TOTAL RECORD COUNT: \" + str(total_count))\n",
        "europe_filtered_count = df.filter(col('Origin')=='Europe').count()\n",
        "print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n",
        "# df.filter(col('Origin')=='Europe').show(truncate=False)\n",
        "# Filtering rows in PySpark based on Multiple conditions\n",
        "total_count = df.count()\n",
        "print(\"TOTAL RECORD COUNT: \" + str(total_count))\n",
        "europe_filtered_count = df.filter((col('Origin')=='Europe') &\n",
        "                                  (col('Cylinders')==4)).count() # Two conditions added here\n",
        "print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n",
        "# df.filter(col('Origin')=='Europe').show(truncate=False)\n",
        "\n",
        "#Get Unique Rows in PySpark\n",
        "# df.select('Origin').distinct().show()\n",
        "\n",
        "\n",
        "\n",
        "#Get Unique Rows in PySpark based on mutliple columns\n",
        "# df.select('Origin','model').distinct().show()\n",
        "\n",
        "\n",
        "\n",
        "# Sort Rows in PySpark\n",
        "# By default the data will be sorted in ascending order\n",
        "# df.orderBy('Cylinders').show(5, truncate=False)\n",
        "# df.orderBy('Horsepower', ascending=False).show(5, truncate=False)\n",
        "\n",
        "# Using groupBy aand orderBy together\n",
        "df.groupBy(\"Origin\").count().orderBy('count', ascending=False).show(10)\n",
        "\n",
        "# CASE 1: Union When columns are in order\n",
        "europe_cars = df.filter((col('Origin')=='Europe') & (col('Cylinders')==5))\n",
        "japan_cars = df.filter((col('Origin')=='Japan') & (col('Cylinders')==3))\n",
        "print(\"EUROPE CARS: \"+str(europe_cars.count()))\n",
        "print(\"JAPAN CARS: \"+str(japan_cars.count()))\n",
        "print(\"AFTER UNION: \"+str(europe_cars.union(japan_cars).count()))\n",
        "\n",
        "# CASE 1: Union When columns are not in order\n",
        "# Creating two dataframes with jumbled columns\n",
        "df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
        "df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
        "df1.unionByName(df2).show()\n",
        "\n"
      ],
      "metadata": {
        "id": "1bIASJN9KgUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443bf00f-eeda-40db-f3cf-cce0ac1b662f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+\n",
            "|Origin|Model|count|\n",
            "+------+-----+-----+\n",
            "|Europe|   71|    5|\n",
            "|Europe|   80|    9|\n",
            "|Europe|   79|    4|\n",
            "| Japan|   75|    4|\n",
            "|    US|   72|   18|\n",
            "+------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "TOTAL RECORD COUNT: 406\n",
            "EUROPE FILTERED RECORD COUNT: 73\n",
            "TOTAL RECORD COUNT: 406\n",
            "EUROPE FILTERED RECORD COUNT: 66\n",
            "+------+-----+\n",
            "|Origin|count|\n",
            "+------+-----+\n",
            "|    US|  254|\n",
            "| Japan|   79|\n",
            "|Europe|   73|\n",
            "+------+-----+\n",
            "\n",
            "EUROPE CARS: 3\n",
            "JAPAN CARS: 4\n",
            "AFTER UNION: 7\n",
            "+----+----+----+\n",
            "|col0|col1|col2|\n",
            "+----+----+----+\n",
            "|   1|   2|   3|\n",
            "|   6|   4|   5|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OTsQ-3b8B1nO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd638e4e-454d-49a5-a935-a329f381e7f8"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col0|col1|col2|\n",
            "+----+----+----+\n",
            "|   1|   2|   3|\n",
            "|   6|   4|   5|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6RG_-yNBUM9x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}